{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  \"Llamas are members of the camelid family meaning they're pretty closely related to vicuñas and camels\",\n",
    "  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n",
    "  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n",
    "  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n",
    "  \"Llamas are vegetarians and have very efficient digestive systems\",\n",
    "  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\",\n",
    "  \"Astera Labs is designing purpose-built connectivity solutions for AI and Cloud Infastructure\",\n",
    "  \"Astera Labs has a comprehensive portfolio, supporting PCIe, CXL, and Ethernet Solutions\",\n",
    "  \"Astera Labs offers Aries PCIe/CXL Smart DSP Retimers, now sampling the lowest power PCIe 6.x and 3.x Retimers\",\n",
    "  \"Astera Labs offers Leo CXL Smart Memory Controllers, designed for purpose-built memory expanion, sharing, and pooling for AI and cloud platforms",\n",
    "  \"Astera Labs offers Ethernet Smart Cable Modules, delivering flexible, high-performance Active Electrical Cables for AI and Cloud connectivity\",\n",
    "  \"Aries Smart DSP Retimers have typical power draw of 11W to optimize cloud infrastructure TCO and simplifies system design\",\n",
    "  \"Aries Smart DSP Retimers support robust signal integrity with SerDes and DSP customized for demanding AI server channels\",\n",
    "  \"Aries Smart DSP Retimers support enhanced diagnostics & telemetry with COSMOS, offering extended capabilities through in-band and out-of-band management\",\n",
    "  \"Intel Xeon 6 processors features new features like Multiplexed Rank DIMM (MRDIMM) support, Compute Express Link (CXL) enhancements and many others\",\n",
    "  \"Intel Xeon 6 processors allow for the choice of two different CPU microarchitectures: Performance-cores (P-cores) and Efficient-cores (E-cores)\",\n",
    "  \"Intel Xeon 6 has integrated accelerators like Intel® QuickAssist Technology (Intel® QAT), offering even greater performance and efficiency\",\n",
    "  \"Intel Xeon 6900-series processors is designed for maximum performance ideal for the most demanding cloud, AI, and HPC environments\",\n",
    "  \"Intel Xeon 6700-series processors is designed for enhanced performance ideal for a wide array of data center and telco environments\",\n",
    "  \"Intel Xeon 6500-series processors is designed for essential performance ideal for mainstream server and edge environments\",\n",
    "  \"Intel Xeon 6300-series processors is designed for entry-level performance ideal for small/medium business environments\",\n",
    "  \"Intel® Advanced Matrix Extensions (Intel® AMX) speeds up inferencing for INT8 and BF16 and support for FP16-trained models\",\n",
    "  \"Intel® Advanced Matrix Extensions (Intel® AMX) supports  up to 2,048 FLOPS per core for INT8 and 1,024 FLOPS per core for BF16/FP16\",\n",
    "  \"Intel Xeon 6 with Performance-cores supports 128 cores per socket, with up to 504 MB L3 cache, and exceptionally low latency at large L3 access sizes\",\n",
    "  \"Intel Xeon 6 with Efficient-cores supports 288 cores per socket, with as much as 216 MB L3 cache, and with exceptionally low latency at large L3 access sizes\",\n",
    "  \"The Intel® Gaudi® 3 AI Accelerator features two compute dies, which together contain 8 MME engines, 64 TPC engines and 24x 200 Gbps RDMA NIC ports\",\n",
    "  \"The Intel® Gaudi® 3 AI Accelerator features a total of 8 HBM2e chips comprise a 128 GB unified High Bandwidth Memory (HBM\",\n",
    "  \"The Intel® Gaudi® 3 AI Accelerator excels at training and inference with 1.8 PFlops of FP8 and BF16 compute and 3.7 TB/s of HBM bandwidth\",\n",
    "  \"The Intel® Gaudi®3 AI Accelerator OCP Accelerator Module (OAM) Card is offered to system designers in standard OCP OAM 2.0 Mezzanine card form\",\n",
    "  \"The Intel® Gaudi®3 AI Accelerator OCP Accelerator Module (OAM) Card supports up to 900W Total Device Power (TDP) with passive cooling and up to 1.2KW TDP with liquid cooling\",\n",
    "  \"The HLB-325 Universal Baseboard is another product inspired by Open Compute Project (OCP) and offered for simplifying system design with the Intel® Gaudi® 3 AI Accelerator\",\n",
    "  \"The HLB-325 Universal Baseboard supports eight Intel® Gaudi® 3 AI Accelerator cards interconnected in a non-blocking, all-to-all configuration\",\n",
    "  \"The HLB-325 Universal Baseboard uses 21 NICs and routes three 200 GbE NICs from every Intel® Gaudi® 3 AI Accelerator card to six on-board OSFP800 connectors for scaling-out\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes as wvc\n",
    "from weaviate.classes.config import Property, DataType\n",
    "import ollama\n",
    "\n",
    "# Create a new data collection\n",
    "collection = client.collections.create(\n",
    "    name = \"docs2\", # Name of the data collection\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT), # Name and data type of the property\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store each document in a vector embedding database\n",
    "with collection.batch.dynamic() as batch:\n",
    "  for i, d in enumerate(documents):\n",
    "    # Generate embeddings\n",
    "    response = ollama.embeddings(model = \"all-minilm\",\n",
    "                                 prompt = d)\n",
    "\n",
    "    # Add data object with text and embedding\n",
    "    batch.add_object(\n",
    "        properties = {\"text\" : d},\n",
    "        vector = response[\"embedding\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are members of the camelid family meaning they're pretty closely related to vicuñas and camels\n"
     ]
    }
   ],
   "source": [
    "# An example prompt\n",
    "prompt = \"What animals are llamas related to?\"\n",
    "\n",
    "# Generate an embedding for the prompt and retrieve the most relevant doc\n",
    "response = ollama.embeddings(\n",
    "  model = \"all-minilm\",\n",
    "  prompt = prompt,\n",
    ")\n",
    "\n",
    "results = collection.query.near_vector(near_vector = response[\"embedding\"],\n",
    "                                       limit = 1)\n",
    "\n",
    "data = results.objects[0].properties['text']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"Using this data: {data}. Respond to this prompt: {prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided information, llamas are related to:\n",
      "\n",
      "1. Vicuñas\n",
      "2. Camels\n"
     ]
    }
   ],
   "source": [
    "# Generate a response combining the prompt and data we retrieved in step 2\n",
    "output = ollama.generate(\n",
    "  model = \"llama3.1:8b\",\n",
    "  prompt = prompt_template,\n",
    ")\n",
    "\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
